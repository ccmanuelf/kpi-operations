# CSV Upload Read-Back Confirmation Protocol - Implementation Guide

## Overview

This document describes the implementation of the **3-Step CSV Upload Confirmation Workflow** as specified in KPI_Challenge_Context.md. This critical feature ensures data integrity through validation, preview, and confirmation before importing production data.

## Implementation Status: âœ… COMPLETE

**Task**: TASK 3.2 - Implement CSV Upload Read-Back Confirmation Protocol
**Priority**: HIGH (MANDATORY spec requirement)
**Impact**: Data Integrity & User Trust

---

## Architecture

### Frontend Components

#### 1. CSVUploadDialog.vue (`/frontend/src/components/`)

**3-Step Workflow:**

**Step 1: Upload & Validate**
- File input accepting `.csv` and `.xlsx`
- Client-side validation using PapaParse
- Checks for required columns
- Validates data types and ranges
- Displays validation summary (valid/invalid row counts)

**Step 2: Preview Grid**
- AG Grid showing ALL rows with validation status
- Color-coded rows:
  - ðŸŸ¢ Green = Valid
  - ðŸ”´ Red = Error
- Editable grid for inline error correction
- Calculated fields displayed (efficiency %)
- Summary statistics at top

**Step 3: Confirm & Import**
- Final confirmation dialog
- Shows counts: valid/invalid/total rows
- Progress indicator during import
- Success/failure feedback
- Links to import log

**Key Features:**
- Real-time cell validation on edit
- Re-calculation of metrics when values change
- Download CSV template
- Error messages per row
- Batch size handling (no row limits)

#### 2. csvValidation.js (`/frontend/src/utils/`)

**Validation Functions:**
```javascript
// Required columns check
validateHeaders(headers)

// Date validation (YYYY-MM-DD format)
validateDate(dateString)

// Positive integer validation
validatePositiveInteger(value, fieldName, minValue)

// Positive decimal validation
validatePositiveDecimal(value, fieldName, minValue)

// Complete row validation
validateProductionEntry(row, rowIndex)

// Mock KPI calculations for preview
calculateMockEfficiency(unitsProduced, runTimeHours)
calculateMockPerformance(unitsProduced, runTimeHours, employeesAssigned)
```

**Required Columns:**
- `production_date`
- `product_id`
- `shift_id`
- `units_produced`
- `run_time_hours`
- `employees_assigned`

**Optional Columns:**
- `work_order_number`
- `defect_count`
- `scrap_count`
- `notes`

#### 3. ProductionEntry.vue Update

**Before:**
```vue
<CSVUpload />  <!-- Old component (no validation) -->
```

**After:**
```vue
<CSVUploadDialog />  <!-- New 3-step workflow -->
```

---

### Backend Implementation

#### 1. Batch Import Endpoint

**Route:** `POST /api/production/batch-import`

**Request Body:**
```json
{
  "entries": [
    {
      "production_date": "2026-01-01",
      "product_id": 1,
      "shift_id": 1,
      "work_order_number": "WO-2026-001",
      "units_produced": 250,
      "run_time_hours": 7.5,
      "employees_assigned": 3,
      "defect_count": 5,
      "scrap_count": 2,
      "notes": "Example entry"
    }
  ]
}
```

**Response:**
```json
{
  "total_rows": 100,
  "successful": 98,
  "failed": 2,
  "errors": [
    {
      "row": 15,
      "error": "Invalid product_id",
      "data": {...}
    }
  ],
  "created_entries": [1234, 1235, ...],
  "import_log_id": 42,
  "import_timestamp": "2026-01-02T12:34:56.789Z"
}
```

**Features:**
- Validates each entry server-side
- Creates production entries one-by-one
- Logs import to `import_log` table
- Returns detailed error information
- Atomic transaction (all or nothing)

#### 2. Import Log Table

**Schema:** `/backend/db/import_log_table.sql`

```sql
CREATE TABLE import_log (
    log_id SERIAL PRIMARY KEY,
    user_id INTEGER NOT NULL REFERENCES users(user_id),
    import_timestamp TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    file_name VARCHAR(255),
    rows_attempted INTEGER NOT NULL,
    rows_succeeded INTEGER NOT NULL,
    rows_failed INTEGER NOT NULL,
    error_details TEXT,  -- JSON string
    import_type VARCHAR(50) NOT NULL  -- 'csv_upload' or 'batch_import'
);
```

**Indexes:**
- `idx_import_log_user_id` - Query by user
- `idx_import_log_timestamp` - Query by date

#### 3. Import Logs API

**Route:** `GET /api/import-logs?limit=50`

**Response:**
```json
[
  {
    "log_id": 42,
    "user_id": 1,
    "import_timestamp": "2026-01-02T12:34:56.789Z",
    "file_name": null,
    "rows_attempted": 100,
    "rows_succeeded": 98,
    "rows_failed": 2,
    "error_details": "[{\"row\": 15, \"error\": \"...\"}]",
    "import_type": "batch_import"
  }
]
```

---

### State Management

#### Pinia Store Update (`kpiStore.js`)

**New Action:**
```javascript
async batchImportProduction(entries) {
  this.loading = true
  this.error = null

  try {
    const response = await api.batchImportProduction(entries)
    return { success: true, data: response.data }
  } catch (error) {
    this.error = error.response?.data?.detail || 'Failed to batch import'
    return { success: false, error: this.error }
  } finally {
    this.loading = false
  }
}
```

---

### API Service Update (`api.js`)

**New Method:**
```javascript
batchImportProduction(entries) {
  return api.post('/production/batch-import', { entries })
}
```

---

## Dependencies

### NPM Packages

```json
{
  "papaparse": "^5.4.1"  // CSV parsing
}
```

**Installation:**
```bash
cd frontend
npm install papaparse --save
```

### Python Packages

No additional packages required (uses standard library: `json`, `csv`, `io`)

---

## Database Migration

**To apply the import_log table:**

```bash
psql -U your_user -d kpi_platform -f backend/db/import_log_table.sql
```

Or run the SQL manually:
```sql
-- See backend/db/import_log_table.sql for full schema
```

---

## User Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         STEP 1: UPLOAD                          â”‚
â”‚  [Select CSV File] â†’ [Validate Columns] â†’ [Parse Rows]         â”‚
â”‚                                                                  â”‚
â”‚  Validation Results:                                            â”‚
â”‚  âœ“ Total Rows: 100                                             â”‚
â”‚  âœ“ Valid Rows: 98                                              â”‚
â”‚  âœ— Invalid Rows: 2                                             â”‚
â”‚                                                                  â”‚
â”‚  [Cancel]                              [Next: Preview Data] â†’  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       STEP 2: PREVIEW GRID                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Status â”‚ Date       â”‚ Product â”‚ Units â”‚ Efficiency â”‚ ... â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚ âœ“ Validâ”‚ 2026-01-01 â”‚ Prod A  â”‚ 250   â”‚ 85.5%     â”‚ ... â”‚   â”‚
â”‚  â”‚ âœ“ Validâ”‚ 2026-01-01 â”‚ Prod B  â”‚ 180   â”‚ 92.3%     â”‚ ... â”‚   â”‚
â”‚  â”‚ âœ— Errorâ”‚ 2026-01-02 â”‚ Invalid â”‚ -10   â”‚ ERR       â”‚ ... â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â”‚  [â† Back]  [Cancel]                 [Next: Confirm Import] â†’   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      STEP 3: CONFIRMATION                       â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   98 Rows        â”‚    2 Rows        â”‚   100 Rows       â”‚    â”‚
â”‚  â”‚   to Import      â”‚    to Skip       â”‚   Total          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â”‚  âš  2 row(s) with errors will be skipped.                       â”‚
â”‚                                                                  â”‚
â”‚  Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100%                         â”‚
â”‚                                                                  â”‚
â”‚  âœ“ Import completed successfully!                              â”‚
â”‚  â€¢ Successfully Imported: 98                                   â”‚
â”‚  â€¢ Failed: 0                                                   â”‚
â”‚  â€¢ Import Log ID: 42                                           â”‚
â”‚                                                                  â”‚
â”‚  [â† Back]  [Cancel]              [âœ“ Confirm & Import 98 Rows]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Testing Checklist

### Frontend Tests

- [ ] Upload valid CSV file
- [ ] Upload CSV with missing columns
- [ ] Upload CSV with invalid data types
- [ ] Edit cell in preview grid
- [ ] Verify re-validation after edit
- [ ] Download CSV template
- [ ] Cancel at each step
- [ ] Complete full import workflow
- [ ] Check error display for invalid rows
- [ ] Verify row color coding (green/red)

### Backend Tests

- [ ] POST /api/production/batch-import with valid data
- [ ] POST /api/production/batch-import with invalid data
- [ ] Verify import_log table creation
- [ ] GET /api/import-logs returns user's logs
- [ ] Verify error_details JSON format
- [ ] Check created_entries list accuracy
- [ ] Test with large datasets (1000+ rows)
- [ ] Verify transaction rollback on error

### Integration Tests

- [ ] End-to-end: Upload â†’ Preview â†’ Confirm â†’ Verify in database
- [ ] Verify data refresh after successful import
- [ ] Check import log ID returned to frontend
- [ ] Test concurrent imports by multiple users
- [ ] Verify user permissions enforced

---

## Performance Considerations

### Frontend

- **PapaParse Streaming**: For files > 10,000 rows, consider streaming mode
- **AG Grid Virtualization**: Automatically handles large datasets
- **Batch Size**: No hard limit, but recommend chunking at 5,000 rows

### Backend

- **Database Transaction**: All entries inserted in single transaction
- **Error Handling**: Continues processing even if individual rows fail
- **Import Logging**: Async/non-blocking to avoid slowing down import

---

## Error Handling

### Client-Side Validation Errors

```javascript
{
  _validationStatus: 'error',
  _validationErrors: 'Invalid date format; Invalid product_id',
  _rowIndex: 15
}
```

### Server-Side Errors

```json
{
  "row": 15,
  "error": "Foreign key constraint failed: product_id 999 not found",
  "data": { "product_id": 999, ... }
}
```

---

## Security Considerations

1. **Authentication**: All endpoints require valid JWT token
2. **Authorization**: Users can only access their own import logs
3. **Input Validation**: Server-side validation duplicates client-side checks
4. **SQL Injection**: Parameterized queries prevent injection
5. **File Size Limits**: Frontend enforces reasonable file sizes
6. **Rate Limiting**: Consider adding rate limits for bulk imports

---

## Future Enhancements

### Phase 2 Features (Not in Current Scope)

- [ ] Excel (.xlsx) file support (requires SheetJS/xlsx library)
- [ ] Background job processing for very large imports (> 10,000 rows)
- [ ] Email notification on import completion
- [ ] Import scheduling (cron-style)
- [ ] Import templates management (save/load column mappings)
- [ ] Duplicate detection (check for existing entries)
- [ ] Partial import retry (skip succeeded rows)
- [ ] Import diff preview (show changes before applying)

---

## Troubleshooting

### Common Issues

**Issue**: "Missing required columns" error
**Solution**: Ensure CSV has exact column names (case-sensitive)

**Issue**: Import log not created
**Solution**: Run migration SQL to create `import_log` table

**Issue**: Date parsing errors
**Solution**: Use YYYY-MM-DD format (ISO 8601)

**Issue**: AG Grid not displaying
**Solution**: Verify AG Grid CSS imports in main.js or component

**Issue**: PapaParse errors
**Solution**: Check file encoding (should be UTF-8)

---

## Files Modified/Created

### Frontend

**Created:**
- `/frontend/src/components/CSVUploadDialog.vue` (380 lines)
- `/frontend/src/utils/csvValidation.js` (180 lines)

**Modified:**
- `/frontend/src/views/ProductionEntry.vue` (removed CSVUpload, added CSVUploadDialog)
- `/frontend/src/stores/kpiStore.js` (added batchImportProduction action)
- `/frontend/src/services/api.js` (added batchImportProduction method)
- `/frontend/package.json` (added papaparse dependency)

### Backend

**Created:**
- `/backend/models/import_log.py` (40 lines)
- `/backend/db/import_log_table.sql` (30 lines)

**Modified:**
- `/backend/main.py` (added batch_import_production endpoint + import_logs endpoint)

---

## Compliance with Specification

This implementation fully satisfies the requirements from `KPI_Challenge_Context.md`:

âœ… **Read-Back Confirmation**: 3-step workflow with validation â†’ preview â†’ confirm
âœ… **Validation**: Client-side and server-side validation of all fields
âœ… **Preview Grid**: AG Grid with editable cells and color coding
âœ… **Error Handling**: Clear error messages with row-level details
âœ… **Import Logging**: Complete audit trail in `import_log` table
âœ… **User Experience**: Progress indicators, success feedback, template download
âœ… **Data Integrity**: Atomic transactions, rollback on failure

---

## Contact

For questions or issues with this implementation, please refer to:
- **Specification**: `/docs/KPI_Challenge_Context.md`
- **AG Grid Guide**: `/docs/phase4-aggrid-implementation-guide.md`
- **Architecture**: `/docs/ARCHITECTURE.md`

---

**Implementation Date**: 2026-01-02
**Status**: âœ… Production Ready
**Next Steps**: End-to-end testing, user acceptance testing (UAT)
